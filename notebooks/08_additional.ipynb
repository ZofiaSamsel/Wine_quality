{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional\n",
    "This notebook includes thoughts that are not in our main outline and also play the important part in our data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "The summary for the Model_White Wine Poor (Without Outliers):\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                quality   R-squared:                       0.067\n",
      "Model:                            OLS   Adj. R-squared:                  0.061\n",
      "Method:                 Least Squares   F-statistic:                     11.69\n",
      "Date:                Wed, 22 Nov 2023   Prob (F-statistic):           3.67e-16\n",
      "Time:                        16:17:56   Log-Likelihood:                -443.99\n",
      "No. Observations:                1308   AIC:                             906.0\n",
      "Df Residuals:                    1299   BIC:                             952.6\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================\n",
      "                          coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                   4.8454      0.164     29.552      0.000       4.524       5.167\n",
      "free sulfur dioxide     0.0020      0.001      3.483      0.001       0.001       0.003\n",
      "volatile acidity       -0.1660      0.030     -5.582      0.000      -0.224      -0.108\n",
      "residual sugar          0.0043      0.002      2.063      0.039       0.000       0.008\n",
      "alcohol                -0.0194      0.013     -1.502      0.133      -0.045       0.006\n",
      "fixed acidity          -0.0280      0.011     -2.439      0.015      -0.051      -0.005\n",
      "citric acid             0.1147      0.074      1.554      0.120      -0.030       0.259\n",
      "chlorides              -0.0210      0.031     -0.674      0.500      -0.082       0.040\n",
      "sulphates               0.0379      0.096      0.394      0.694      -0.151       0.226\n",
      "==============================================================================\n",
      "Omnibus:                      818.946   Durbin-Watson:                   1.954\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6872.062\n",
      "Skew:                          -2.919   Prob(JB):                         0.00\n",
      "Kurtosis:                      12.592   Cond. No.                         751.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified..\n",
      "\n",
      "-------------------------------------------------------\n",
      "R-squared on Test Set: 0.07019992229960625\n",
      "Mean Squared Error on Test Set: 0.13634486564732126\n",
      "\n",
      "-------------------------------------------------------\n",
      "The summary for the Model_Red Wine Poor (Without Outliers):\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                quality   R-squared:                       0.099\n",
      "Model:                            OLS   Adj. R-squared:                  0.087\n",
      "Method:                 Least Squares   F-statistic:                     8.017\n",
      "Date:                Wed, 22 Nov 2023   Prob (F-statistic):           2.76e-10\n",
      "Time:                        16:17:56   Log-Likelihood:                -180.16\n",
      "No. Observations:                 592   AIC:                             378.3\n",
      "Df Residuals:                     583   BIC:                             417.8\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                   -5.4040     12.471     -0.433      0.665     -29.897      19.089\n",
      "volatile acidity        -0.5369      0.090     -5.985      0.000      -0.713      -0.361\n",
      "total sulfur dioxide     0.0017      0.000      4.119      0.000       0.001       0.003\n",
      "pH                      -0.2327      0.111     -2.093      0.037      -0.451      -0.014\n",
      "citric acid             -0.2948      0.116     -2.538      0.011      -0.523      -0.067\n",
      "alcohol                  0.0139      0.022      0.628      0.530      -0.030       0.058\n",
      "sulphates                0.0283      0.095      0.299      0.765      -0.158       0.215\n",
      "density                 11.2753     12.464      0.905      0.366     -13.204      35.754\n",
      "residual sugar          -0.0408      0.048     -0.855      0.393      -0.134       0.053\n",
      "==============================================================================\n",
      "Omnibus:                      407.201   Durbin-Watson:                   2.014\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3857.604\n",
      "Skew:                          -3.088   Prob(JB):                         0.00\n",
      "Kurtosis:                      13.874   Cond. No.                     8.51e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 8.51e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems..\n",
      "\n",
      "-------------------------------------------------------\n",
      "R-squared on Test Set: 0.13356530134707167\n",
      "Mean Squared Error on Test Set: 0.08250272298330211\n",
      "\n",
      "-------------------------------------------------------\n",
      "The summary for the Model_White Wine Good (Without Outliers):\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                quality   R-squared:                       0.116\n",
      "Model:                            OLS   Adj. R-squared:                  0.113\n",
      "Method:                 Least Squares   F-statistic:                     42.62\n",
      "Date:                Wed, 22 Nov 2023   Prob (F-statistic):           1.61e-64\n",
      "Time:                        16:17:56   Log-Likelihood:                -2161.5\n",
      "No. Observations:                2603   AIC:                             4341.\n",
      "Df Residuals:                    2594   BIC:                             4394.\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                  115.9796     22.978      5.048      0.000      70.923     161.036\n",
      "alcohol                  0.0188      0.028      0.662      0.508      -0.037       0.074\n",
      "density               -113.8570     23.251     -4.897      0.000    -159.449     -68.265\n",
      "chlorides               -0.0868      0.044     -1.995      0.046      -0.172      -0.002\n",
      "total sulfur dioxide     0.0004      0.000      1.076      0.282      -0.000       0.001\n",
      "residual sugar           0.0499      0.009      5.636      0.000       0.033       0.067\n",
      "pH                       0.6328      0.113      5.614      0.000       0.412       0.854\n",
      "fixed acidity            0.0952      0.024      3.979      0.000       0.048       0.142\n",
      "citric acid              0.0427      0.110      0.388      0.698      -0.174       0.259\n",
      "==============================================================================\n",
      "Omnibus:                      451.296   Durbin-Watson:                   1.930\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              724.773\n",
      "Skew:                           1.182   Prob(JB):                    4.15e-158\n",
      "Kurtosis:                       4.047   Cond. No.                     4.18e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.18e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems..\n",
      "\n",
      "-------------------------------------------------------\n",
      "R-squared on Test Set: 0.11930136991823315\n",
      "Mean Squared Error on Test Set: 0.32609047084638937\n",
      "\n",
      "-------------------------------------------------------\n",
      "The summary for the Model_Red Wine Good (Without Outliers):\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                quality   R-squared:                       0.196\n",
      "Model:                            OLS   Adj. R-squared:                  0.187\n",
      "Method:                 Least Squares   F-statistic:                     20.48\n",
      "Date:                Wed, 22 Nov 2023   Prob (F-statistic):           7.36e-28\n",
      "Time:                        16:17:56   Log-Likelihood:                -419.30\n",
      "No. Observations:                 680   AIC:                             856.6\n",
      "Df Residuals:                     671   BIC:                             897.3\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                   10.0697     13.171      0.765      0.445     -15.791      35.930\n",
      "alcohol                  0.1289      0.022      5.962      0.000       0.086       0.171\n",
      "volatile acidity        -0.1880      0.146     -1.292      0.197      -0.474       0.098\n",
      "sulphates                0.6421      0.137      4.682      0.000       0.373       0.911\n",
      "citric acid              0.1350      0.149      0.904      0.366      -0.158       0.428\n",
      "chlorides               -0.1498      0.063     -2.375      0.018      -0.274      -0.026\n",
      "total sulfur dioxide    -0.0018      0.001     -2.400      0.017      -0.003      -0.000\n",
      "density                 -5.0362     13.081     -0.385      0.700     -30.722      20.649\n",
      "pH                      -0.2665      0.150     -1.772      0.077      -0.562       0.029\n",
      "==============================================================================\n",
      "Omnibus:                      113.755   Durbin-Watson:                   1.973\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              174.803\n",
      "Skew:                           1.108   Prob(JB):                     1.10e-38\n",
      "Kurtosis:                       4.121   Cond. No.                     4.95e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.95e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems..\n",
      "\n",
      "-------------------------------------------------------\n",
      "R-squared on Test Set: 0.28466886558862003\n",
      "Mean Squared Error on Test Set: 0.147073314184919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run 06_model_building.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gridsearch Model creation\n",
    "\n",
    "Firstly, we decided to create the Grid seach with cross-validation to find the best model for each of our datasets, but after realising that SciKit Learn doesn't provide statistical analysis of the model we had to  recreate the Linear Model using scipy library.\n",
    "\n",
    "###  Function: evaluate_GS_model\n",
    "\n",
    "**Parameters:**\n",
    "*   `pipe` - the pipeline through which we pass\n",
    "*   `X_train`, `y_train`, `X_test`, `y_test` - data\n",
    "*   `classifier_params` - hyperparameters for the evaluated model\n",
    "*   `pipeline_name`\n",
    "*   `cv` - type of cross-validation (StratifiedKFold 5-fold)\n",
    "\n",
    "We define a `GridSearchCV` refitting on `r2`.\n",
    "- Fit the model.\n",
    "- Predict test data.\n",
    "- Extract the mean `r2` from cross-validation.\n",
    "- Select the model with the best parameters.\n",
    "- Save the results to a dataframe.\n",
    "- Calculate the score matrix **(since SciKit Learn doesn't provide the statistical analysis of the model we tried to recreate it using stats library)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GS_model(\n",
    "    pipe,\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    regression_params,\n",
    "    pipeline_name,\n",
    "    cv=KFold(n_splits=5),\n",
    "    predict_test=True,\n",
    "    predict_train=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a machine learning model using grid search with cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    - pipe (sklearn.pipeline.Pipeline): The machine learning pipeline to evaluate.\n",
    "    - X_train, y_train, X_test, y_test (pandas.DataFrame): Training and testing datasets.\n",
    "    - regression_params (dict): Parameter grid for grid search.\n",
    "    - pipeline_name (str): Name of the machine learning pipeline.\n",
    "    - cv (sklearn.model_selection._split): Cross-validation strategy (default: KFold with 5 splits).\n",
    "    - predict_test (bool): Whether to predict and evaluate on the test set.\n",
    "    - predict_train (bool): Whether to predict and evaluate on the training set.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Results DataFrame and Evaluation Matrix DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # define grid search\n",
    "    grid_search_model = GridSearchCV(\n",
    "        pipe,\n",
    "        regression_params,\n",
    "        cv=cv,\n",
    "        scoring=[\"r2\", \"neg_mean_squared_error\", \"neg_median_absolute_error\"],\n",
    "        refit=\"neg_mean_squared_error\",\n",
    "        return_train_score=True,\n",
    "        verbose=3,\n",
    "    )\n",
    "\n",
    "    # fit model\n",
    "    grid_search_model.fit(X_train, y_train)\n",
    "    y_test_pred = grid_search_model.predict(X_test) if predict_test else None\n",
    "    test_score = metrics.mean_squared_error(y_test, y_test_pred) if predict_test else None\n",
    "\n",
    "    y_train_pred = grid_search_model.predict(X_train) if predict_train else None\n",
    "    train_score = metrics.mean_squared_error(y_train, y_train_pred) if predict_train else None \n",
    "\n",
    "    mean_cv_score = grid_search_model.best_score_\n",
    "\n",
    "    cv_results_df = pd.DataFrame(grid_search_model.cv_results_).iloc[[grid_search_model.best_index_]]\n",
    "    cv_splits_scores_df = cv_results_df.filter(regex=r\"split\\d*_test_mean_squared_error\").reset_index(drop=True) \n",
    "    \n",
    "    # getting scores based on the metrics defined in the score\n",
    "    metrics_results_df = cv_results_df.filter(regex=r\"mean_test_*\").reset_index(drop=True)\n",
    "\n",
    "    best_estimator = grid_search_model.best_estimator_.named_steps[pipe.steps[-1][0]]\n",
    "\n",
    "    coefficients = best_estimator.coef_\n",
    "    intercept = best_estimator.intercept_\n",
    "\n",
    "    # save results in DataFrame\n",
    "    this_result = pd.concat(\n",
    "        [pd.DataFrame({\n",
    "            \"pipeline_name\": [pipeline_name],\n",
    "            \"features\": [list(X_test.columns)],\n",
    "            'coef': [coefficients], \n",
    "            \"train score\": [train_score],\n",
    "            \"mean_cv_score\": [mean_cv_score],\n",
    "            \"test_score\": [test_score],\n",
    "            \"best_model\": [grid_search_model.best_estimator_],\n",
    "            \"parameters\": [grid_search_model.best_params_],\n",
    "            }),\n",
    "            cv_splits_scores_df,\n",
    "            metrics_results_df,\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # create DataFrame for the evaluation matrix\n",
    "    parameters = np.append(intercept, coefficients)\n",
    "    dataset_column_names = ['Intercept']\n",
    "    dataset_column_names.extend(list(X_test.columns))\n",
    "    \n",
    "    evaluation_matrix_df = pd.DataFrame({\n",
    "        'df': [pipeline_name for number in range(len(parameters))],\n",
    "        'const': [pipeline_name for number in range(len(parameters))],\n",
    "        \"Parameters\": pd.Series(dataset_column_names),\n",
    "        \"Coefficients\": pd.Series(parameters),\n",
    "    })\n",
    "    \n",
    "    return this_result, evaluation_matrix_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Linear Models using pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For **y** we take the labels representing the quality of wine form 1 to 10 (1 the lowest score)\n",
    "* For **X** we select the 8 psychomical features.\n",
    "* The three pairs of the features that were highly correlated we chose the former, the later or the mean of both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for our gridsearch\n",
    "base_steps = [('scaler', StandardScaler())] #normalizing values\n",
    "lr = (\"lr\", LinearRegression())\n",
    "lr_params = dict(lr__fit_intercept=[True, False])\n",
    "\n",
    "estimators = [(lr, lr_params)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_pipelines(df_lists, features=[top_features_dict, features_among_highly_associated_dict], test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Create pipelines and evaluate models for each dataset and feature set.\n",
    "\n",
    "    Parameters:\n",
    "    - df_lists (list): List of datasets.\n",
    "    - features (list): List of feature sets to use in the pipelines.\n",
    "    - test_size (float): Size of the test set in the train-test split (default: 0.2).\n",
    "    - random_state (int): Random seed for reproducibility (default: 42).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Pipeline, DataFrame with scores, and a list of evaluation matrices.\n",
    "    \"\"\"\n",
    "    results_df = pd.DataFrame()\n",
    "    evaluation_matrix_list = []\n",
    "\n",
    "    # iterate through each dataset\n",
    "    for dataset in df_lists:\n",
    "        dataset_name = get_wine_str(dataset)\n",
    "        \n",
    "        # iterate through each feature list \n",
    "        for feature_set in features:\n",
    "            # name the pipeline\n",
    "            pipeline_name = f\"Pipeline_{dataset_name}\"\n",
    "\n",
    "            # create X and y sets\n",
    "            y = dataset['quality']\n",
    "            X = dataset[feature_set[dataset_name]]\n",
    "\n",
    "            # train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, \n",
    "                y, \n",
    "                test_size=test_size, \n",
    "                random_state=random_state\n",
    "            )\n",
    "\n",
    "            # GridSearch \n",
    "            for (estimator, params) in estimators:\n",
    "                print(f\"Evaluating {estimator}\\n\")\n",
    "\n",
    "                # create a pipeline from the base steps list and estimator\n",
    "                pipe = Pipeline(base_steps + [estimator])\n",
    "\n",
    "                this_results, evaluation_matrix = evaluate_GS_model(\n",
    "                    pipe=pipe,\n",
    "                    pipeline_name=pipeline_name,\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test,\n",
    "                    regression_params=params,\n",
    "                    predict_test=True,\n",
    "                    predict_train=True,\n",
    "                    cv=StratifiedKFold(n_splits=5)\n",
    "                )\n",
    "                results_df = pd.concat([results_df, this_results], ignore_index=True)\n",
    "\n",
    "                evaluation_matrix_list.append([evaluation_matrix])\n",
    "\n",
    "    return pipe, results_df, evaluation_matrix_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing pipeline on our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating ('lr', LinearRegression()) \n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.113, test=-0.116) neg_median_absolute_error: (train=-0.113, test=-0.117) r2: (train=0.079, test=0.089) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.113, test=-0.118) neg_median_absolute_error: (train=-0.112, test=-0.110) r2: (train=0.082, test=0.073) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.112, test=-0.121) neg_median_absolute_error: (train=-0.113, test=-0.102) r2: (train=0.082, test=0.068) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.114, test=-0.115) neg_median_absolute_error: (train=-0.113, test=-0.128) r2: (train=0.093, test=0.017) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.115, test=-0.108) neg_median_absolute_error: (train=-0.116, test=-0.099) r2: (train=0.081, test=0.080) total time=   0.0s\n",
      "[CV 1/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-23.951, test=-23.974) neg_median_absolute_error: (train=-4.962, test=-4.975) r2: (train=-193.909, test=-187.441) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-23.951, test=-23.943) neg_median_absolute_error: (train=-4.967, test=-4.971) r2: (train=-193.906, test=-187.196) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-23.959, test=-23.811) neg_median_absolute_error: (train=-4.969, test=-4.953) r2: (train=-195.144, test=-181.995) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-23.943, test=-24.073) neg_median_absolute_error: (train=-4.965, test=-4.983) r2: (train=-189.907, test=-204.653) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-23.945, test=-23.924) neg_median_absolute_error: (train=-4.966, test=-4.960) r2: (train=-189.919, test=-203.378) total time=   0.0s\n",
      "Rating ('lr', LinearRegression()) \n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.115, test=-0.117) neg_median_absolute_error: (train=-0.120, test=-0.126) r2: (train=0.062, test=0.083) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.115, test=-0.119) neg_median_absolute_error: (train=-0.121, test=-0.121) r2: (train=0.067, test=0.062) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.114, test=-0.120) neg_median_absolute_error: (train=-0.122, test=-0.112) r2: (train=0.063, test=0.075) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.115, test=-0.117) neg_median_absolute_error: (train=-0.117, test=-0.120) r2: (train=0.080, test=-0.003) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.116, test=-0.112) neg_median_absolute_error: (train=-0.119, test=-0.115) r2: (train=0.072, test=0.042) total time=   0.0s\n",
      "[CV 1/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-23.953, test=-23.958) neg_median_absolute_error: (train=-4.974, test=-4.978) r2: (train=-193.926, test=-187.314) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-23.953, test=-23.975) neg_median_absolute_error: (train=-4.970, test=-4.982) r2: (train=-193.921, test=-187.447) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-23.962, test=-23.856) neg_median_absolute_error: (train=-4.976, test=-4.965) r2: (train=-195.164, test=-182.338) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-23.945, test=-24.016) neg_median_absolute_error: (train=-4.966, test=-4.979) r2: (train=-189.920, test=-204.165) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-23.946, test=-23.933) neg_median_absolute_error: (train=-4.973, test=-4.960) r2: (train=-189.928, test=-203.455) total time=   0.0s\n",
      "Rating ('lr', LinearRegression()) \n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.307, test=-0.316) neg_median_absolute_error: (train=-0.394, test=-0.403) r2: (train=0.119, test=0.101) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.305, test=-0.321) neg_median_absolute_error: (train=-0.393, test=-0.402) r2: (train=0.122, test=0.087) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.309, test=-0.306) neg_median_absolute_error: (train=-0.395, test=-0.395) r2: (train=0.112, test=0.130) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.311, test=-0.299) neg_median_absolute_error: (train=-0.393, test=-0.374) r2: (train=0.114, test=0.120) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.308, test=-0.307) neg_median_absolute_error: (train=-0.393, test=-0.410) r2: (train=0.116, test=0.116) total time=   0.0s\n",
      "[CV 1/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-41.059, test=-41.015) neg_median_absolute_error: (train=-6.193, test=-6.187) r2: (train=-117.012, test=-115.627) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-41.052, test=-41.138) neg_median_absolute_error: (train=-6.193, test=-6.188) r2: (train=-117.029, test=-115.828) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-41.055, test=-41.067) neg_median_absolute_error: (train=-6.180, test=-6.193) r2: (train=-117.039, test=-115.628) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-41.073, test=-41.279) neg_median_absolute_error: (train=-6.178, test=-6.195) r2: (train=-116.042, test=-120.540) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-41.065, test=-40.801) neg_median_absolute_error: (train=-6.186, test=-6.158) r2: (train=-116.699, test=-116.318) total time=   0.0s\n",
      "Rating ('lr', LinearRegression()) \n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.308, test=-0.313) neg_median_absolute_error: (train=-0.402, test=-0.405) r2: (train=0.114, test=0.110) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.306, test=-0.322) neg_median_absolute_error: (train=-0.393, test=-0.402) r2: (train=0.121, test=0.084) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.309, test=-0.308) neg_median_absolute_error: (train=-0.403, test=-0.398) r2: (train=0.111, test=0.125) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.312, test=-0.299) neg_median_absolute_error: (train=-0.400, test=-0.386) r2: (train=0.112, test=0.118) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.308, test=-0.311) neg_median_absolute_error: (train=-0.400, test=-0.416) r2: (train=0.116, test=0.105) total time=   0.0s\n",
      "[CV 1/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-41.061, test=-40.973) neg_median_absolute_error: (train=-6.181, test=-6.165) r2: (train=-117.017, test=-115.508) total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zosia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\zosia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:737: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-41.052, test=-41.174) neg_median_absolute_error: (train=-6.185, test=-6.183) r2: (train=-117.031, test=-115.931) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-41.055, test=-41.141) neg_median_absolute_error: (train=-6.175, test=-6.189) r2: (train=-117.040, test=-115.838) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-41.074, test=-41.249) neg_median_absolute_error: (train=-6.178, test=-6.190) r2: (train=-116.044, test=-120.454) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-41.065, test=-40.775) neg_median_absolute_error: (train=-6.175, test=-6.164) r2: (train=-116.698, test=-116.245) total time=   0.0s\n",
      "Rating ('lr', LinearRegression()) \n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.108, test=-0.109) neg_median_absolute_error: (train=-0.107, test=-0.098) r2: (train=0.074, test=0.168) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.105, test=-0.120) neg_median_absolute_error: (train=-0.108, test=-0.105) r2: (train=0.098, test=0.085) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.112, test=-0.092) neg_median_absolute_error: (train=-0.103, test=-0.115) r2: (train=0.096, test=0.097) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.113, test=-0.089) neg_median_absolute_error: (train=-0.107, test=-0.086) r2: (train=0.092, test=0.119) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.099, test=-0.150) neg_median_absolute_error: (train=-0.109, test=-0.104) r2: (train=0.153, test=-0.138) total time=   0.0s\n",
      "[CV 1/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-24.103, test=-24.027) neg_median_absolute_error: (train=-4.979, test=-4.979) r2: (train=-205.805, test=-182.519) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-24.101, test=-24.176) neg_median_absolute_error: (train=-4.978, test=-4.983) r2: (train=-205.781, test=-183.659) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-24.068, test=-24.153) neg_median_absolute_error: (train=-4.970, test=-4.989) r2: (train=-193.265, test=-237.012) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-24.069, test=-24.062) neg_median_absolute_error: (train=-4.980, test=-4.969) r2: (train=-193.269, test=-236.116) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-24.096, test=-24.000) neg_median_absolute_error: (train=-4.965, test=-4.952) r2: (train=-206.141, test=-180.912) total time=   0.0s\n",
      "Rating ('lr', LinearRegression()) \n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.108, test=-0.109) neg_median_absolute_error: (train=-0.108, test=-0.101) r2: (train=0.076, test=0.165) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.105, test=-0.122) neg_median_absolute_error: (train=-0.108, test=-0.104) r2: (train=0.101, test=0.070) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.112, test=-0.093) neg_median_absolute_error: (train=-0.104, test=-0.116) r2: (train=0.099, test=0.083) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.112, test=-0.089) neg_median_absolute_error: (train=-0.107, test=-0.085) r2: (train=0.092, test=0.122) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.099, test=-0.152) neg_median_absolute_error: (train=-0.105, test=-0.106) r2: (train=0.152, test=-0.151) total time=   0.0s\n",
      "[CV 1/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-24.103, test=-24.045) neg_median_absolute_error: (train=-4.980, test=-4.979) r2: (train=-205.802, test=-182.654) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-24.100, test=-24.131) neg_median_absolute_error: (train=-4.977, test=-4.984) r2: (train=-205.777, test=-183.316) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-24.068, test=-24.181) neg_median_absolute_error: (train=-4.970, test=-4.982) r2: (train=-193.262, test=-237.280) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-24.069, test=-24.052) neg_median_absolute_error: (train=-4.978, test=-4.968) r2: (train=-193.269, test=-236.016) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-24.096, test=-24.007) neg_median_absolute_error: (train=-4.965, test=-4.958) r2: (train=-206.141, test=-180.964) total time=   0.0s\n",
      "Rating ('lr', LinearRegression()) \n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.200, test=-0.212) neg_median_absolute_error: (train=-0.321, test=-0.284) r2: (train=0.206, test=0.126) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.206, test=-0.187) neg_median_absolute_error: (train=-0.322, test=-0.325) r2: (train=0.183, test=0.230) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.204, test=-0.192) neg_median_absolute_error: (train=-0.322, test=-0.271) r2: (train=0.189, test=0.207) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.199, test=-0.217) neg_median_absolute_error: (train=-0.304, test=-0.371) r2: (train=0.193, test=0.177) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.196, test=-0.231) neg_median_absolute_error: (train=-0.299, test=-0.324) r2: (train=0.209, test=0.111) total time=   0.0s\n",
      "[CV 1/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-39.608, test=-39.506) neg_median_absolute_error: (train=-6.165, test=-6.181) r2: (train=-156.177, test=-162.140) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-39.614, test=-39.401) neg_median_absolute_error: (train=-6.161, test=-6.145) r2: (train=-156.199, test=-161.706) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-39.612, test=-39.814) neg_median_absolute_error: (train=-6.159, test=-6.177) r2: (train=-156.194, test=-163.413) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-39.561, test=-39.274) neg_median_absolute_error: (train=-6.170, test=-6.129) r2: (train=-159.378, test=-148.131) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-39.581, test=-39.966) neg_median_absolute_error: (train=-6.164, test=-6.186) r2: (train=-158.922, test=-152.619) total time=   0.0s\n",
      "Rating ('lr', LinearRegression()) \n",
      "\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[CV 1/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.200, test=-0.210) neg_median_absolute_error: (train=-0.313, test=-0.292) r2: (train=0.207, test=0.132) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.205, test=-0.187) neg_median_absolute_error: (train=-0.312, test=-0.326) r2: (train=0.187, test=0.229) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.204, test=-0.191) neg_median_absolute_error: (train=-0.316, test=-0.266) r2: (train=0.192, test=0.211) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.199, test=-0.214) neg_median_absolute_error: (train=-0.303, test=-0.361) r2: (train=0.195, test=0.187) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=True; neg_mean_squared_error: (train=-0.195, test=-0.231) neg_median_absolute_error: (train=-0.295, test=-0.313) r2: (train=0.213, test=0.111) total time=   0.0s\n",
      "[CV 1/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-39.608, test=-39.494) neg_median_absolute_error: (train=-6.166, test=-6.185) r2: (train=-156.175, test=-162.092) total time=   0.0s\n",
      "[CV 2/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-39.613, test=-39.405) neg_median_absolute_error: (train=-6.160, test=-6.151) r2: (train=-156.195, test=-161.723) total time=   0.0s\n",
      "[CV 3/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-39.612, test=-39.811) neg_median_absolute_error: (train=-6.161, test=-6.169) r2: (train=-156.191, test=-163.399) total time=   0.0s\n",
      "[CV 4/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-39.560, test=-39.312) neg_median_absolute_error: (train=-6.160, test=-6.119) r2: (train=-159.376, test=-148.273) total time=   0.0s\n",
      "[CV 5/5] END lr__fit_intercept=False; neg_mean_squared_error: (train=-39.580, test=-39.955) neg_median_absolute_error: (train=-6.163, test=-6.188) r2: (train=-158.918, test=-152.576) total time=   0.0s\n"
     ]
    }
   ],
   "source": [
    "pipe, results_df, evaluation_matrix_list = building_piplines(wine_quality_without_outliers_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>features</th>\n",
       "      <th>coef</th>\n",
       "      <th>train score</th>\n",
       "      <th>mean_cv_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>best_model</th>\n",
       "      <th>parameters</th>\n",
       "      <th>mean_test_r2</th>\n",
       "      <th>mean_test_neg_mean_squared_error</th>\n",
       "      <th>mean_test_neg_median_absolute_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>[free sulfur dioxide, volatile acidity, total ...</td>\n",
       "      <td>[0.009804424160145675, -0.059479220409756464, ...</td>\n",
       "      <td>0.113651</td>\n",
       "      <td>-0.115599</td>\n",
       "      <td>0.136677</td>\n",
       "      <td>(StandardScaler(), LinearRegression())</td>\n",
       "      <td>{'lr__fit_intercept': True}</td>\n",
       "      <td>0.065193</td>\n",
       "      <td>-0.115599</td>\n",
       "      <td>-0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>[free sulfur dioxide, volatile acidity, residu...</td>\n",
       "      <td>[0.037149606235627075, -0.05443671297011143, 0...</td>\n",
       "      <td>0.115442</td>\n",
       "      <td>-0.117185</td>\n",
       "      <td>0.136345</td>\n",
       "      <td>(StandardScaler(), LinearRegression())</td>\n",
       "      <td>{'lr__fit_intercept': True}</td>\n",
       "      <td>0.051781</td>\n",
       "      <td>-0.117185</td>\n",
       "      <td>-0.118854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pipeline_White Wine Good (Without Outliers)</td>\n",
       "      <td>[alcohol, density, chlorides, total sulfur dio...</td>\n",
       "      <td>[0.02335745624729898, -0.33332421629431724, -0...</td>\n",
       "      <td>0.308168</td>\n",
       "      <td>-0.310040</td>\n",
       "      <td>0.326090</td>\n",
       "      <td>(StandardScaler(), LinearRegression())</td>\n",
       "      <td>{'lr__fit_intercept': True}</td>\n",
       "      <td>0.110858</td>\n",
       "      <td>-0.310040</td>\n",
       "      <td>-0.396855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pipeline_White Wine Good (Without Outliers)</td>\n",
       "      <td>[alcohol, chlorides, total sulfur dioxide, res...</td>\n",
       "      <td>[0.20322606485574743, -0.03970062880296661, 0....</td>\n",
       "      <td>0.308824</td>\n",
       "      <td>-0.310859</td>\n",
       "      <td>0.328447</td>\n",
       "      <td>(StandardScaler(), LinearRegression())</td>\n",
       "      <td>{'lr__fit_intercept': True}</td>\n",
       "      <td>0.108493</td>\n",
       "      <td>-0.310859</td>\n",
       "      <td>-0.401499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pipeline_Red Wine Poor (Without Outliers)</td>\n",
       "      <td>[volatile acidity, total sulfur dioxide, pH, f...</td>\n",
       "      <td>[-0.0984315648917267, 0.06506705592099327, -0....</td>\n",
       "      <td>0.107697</td>\n",
       "      <td>-0.111995</td>\n",
       "      <td>0.083575</td>\n",
       "      <td>(StandardScaler(), LinearRegression())</td>\n",
       "      <td>{'lr__fit_intercept': True}</td>\n",
       "      <td>0.066067</td>\n",
       "      <td>-0.111995</td>\n",
       "      <td>-0.101692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pipeline_Red Wine Poor (Without Outliers)</td>\n",
       "      <td>[volatile acidity, total sulfur dioxide, pH, c...</td>\n",
       "      <td>[-0.09738634750601528, 0.06258791750627782, -0...</td>\n",
       "      <td>0.107612</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>0.082503</td>\n",
       "      <td>(StandardScaler(), LinearRegression())</td>\n",
       "      <td>{'lr__fit_intercept': True}</td>\n",
       "      <td>0.057932</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.102416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pipeline_Red Wine Good (Without Outliers)</td>\n",
       "      <td>[alcohol, volatile acidity, sulphates, citric ...</td>\n",
       "      <td>[0.12993460890703012, -0.030543707212247334, 0...</td>\n",
       "      <td>0.201659</td>\n",
       "      <td>-0.207719</td>\n",
       "      <td>0.147937</td>\n",
       "      <td>(StandardScaler(), LinearRegression())</td>\n",
       "      <td>{'lr__fit_intercept': True}</td>\n",
       "      <td>0.169862</td>\n",
       "      <td>-0.207719</td>\n",
       "      <td>-0.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pipeline_Red Wine Good (Without Outliers)</td>\n",
       "      <td>[alcohol, volatile acidity, sulphates, citric ...</td>\n",
       "      <td>[0.14322209473124037, -0.03049374464085257, 0....</td>\n",
       "      <td>0.200962</td>\n",
       "      <td>-0.206721</td>\n",
       "      <td>0.147073</td>\n",
       "      <td>(StandardScaler(), LinearRegression())</td>\n",
       "      <td>{'lr__fit_intercept': True}</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>-0.206721</td>\n",
       "      <td>-0.311507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 pipeline_name  \\\n",
       "0  Pipeline_White Wine Poor (Without Outliers)   \n",
       "1  Pipeline_White Wine Poor (Without Outliers)   \n",
       "2  Pipeline_White Wine Good (Without Outliers)   \n",
       "3  Pipeline_White Wine Good (Without Outliers)   \n",
       "4    Pipeline_Red Wine Poor (Without Outliers)   \n",
       "5    Pipeline_Red Wine Poor (Without Outliers)   \n",
       "6    Pipeline_Red Wine Good (Without Outliers)   \n",
       "7    Pipeline_Red Wine Good (Without Outliers)   \n",
       "\n",
       "                                            features  \\\n",
       "0  [free sulfur dioxide, volatile acidity, total ...   \n",
       "1  [free sulfur dioxide, volatile acidity, residu...   \n",
       "2  [alcohol, density, chlorides, total sulfur dio...   \n",
       "3  [alcohol, chlorides, total sulfur dioxide, res...   \n",
       "4  [volatile acidity, total sulfur dioxide, pH, f...   \n",
       "5  [volatile acidity, total sulfur dioxide, pH, c...   \n",
       "6  [alcohol, volatile acidity, sulphates, citric ...   \n",
       "7  [alcohol, volatile acidity, sulphates, citric ...   \n",
       "\n",
       "                                                coef  train score  \\\n",
       "0  [0.009804424160145675, -0.059479220409756464, ...     0.113651   \n",
       "1  [0.037149606235627075, -0.05443671297011143, 0...     0.115442   \n",
       "2  [0.02335745624729898, -0.33332421629431724, -0...     0.308168   \n",
       "3  [0.20322606485574743, -0.03970062880296661, 0....     0.308824   \n",
       "4  [-0.0984315648917267, 0.06506705592099327, -0....     0.107697   \n",
       "5  [-0.09738634750601528, 0.06258791750627782, -0...     0.107612   \n",
       "6  [0.12993460890703012, -0.030543707212247334, 0...     0.201659   \n",
       "7  [0.14322209473124037, -0.03049374464085257, 0....     0.200962   \n",
       "\n",
       "   mean_cv_score  test_score                              best_model  \\\n",
       "0      -0.115599    0.136677  (StandardScaler(), LinearRegression())   \n",
       "1      -0.117185    0.136345  (StandardScaler(), LinearRegression())   \n",
       "2      -0.310040    0.326090  (StandardScaler(), LinearRegression())   \n",
       "3      -0.310859    0.328447  (StandardScaler(), LinearRegression())   \n",
       "4      -0.111995    0.083575  (StandardScaler(), LinearRegression())   \n",
       "5      -0.113002    0.082503  (StandardScaler(), LinearRegression())   \n",
       "6      -0.207719    0.147937  (StandardScaler(), LinearRegression())   \n",
       "7      -0.206721    0.147073  (StandardScaler(), LinearRegression())   \n",
       "\n",
       "                    parameters  mean_test_r2  \\\n",
       "0  {'lr__fit_intercept': True}      0.065193   \n",
       "1  {'lr__fit_intercept': True}      0.051781   \n",
       "2  {'lr__fit_intercept': True}      0.110858   \n",
       "3  {'lr__fit_intercept': True}      0.108493   \n",
       "4  {'lr__fit_intercept': True}      0.066067   \n",
       "5  {'lr__fit_intercept': True}      0.057932   \n",
       "6  {'lr__fit_intercept': True}      0.169862   \n",
       "7  {'lr__fit_intercept': True}      0.173800   \n",
       "\n",
       "   mean_test_neg_mean_squared_error  mean_test_neg_median_absolute_error  \n",
       "0                         -0.115599                            -0.111200  \n",
       "1                         -0.117185                            -0.118854  \n",
       "2                         -0.310040                            -0.396855  \n",
       "3                         -0.310859                            -0.401499  \n",
       "4                         -0.111995                            -0.101692  \n",
       "5                         -0.113002                            -0.102416  \n",
       "6                         -0.207719                            -0.314700  \n",
       "7                         -0.206721                            -0.311507  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "For each of datasets we identify the most suitable dataset: \n",
    "\n",
    "**White Poor Wine (N=1639)** (1)\n",
    "* r2 on the test = 0.136345\n",
    "* Features: Volatile Acidity, Free Sulfur Dioxide, Residual Sugar, Alcohol, Total Sulfur Dioxide, Fixed Acidity, Density, Citric Acid.\n",
    "\n",
    "**White Good Wine (N=3249)** (3)\n",
    "* r2 on the test = 0.328447\n",
    "* Features: Alcohol, Density, Chlorides, Total Sulfur Dioxide, Residual Sugar, pH, Fixed Acidity\n",
    "\n",
    "**Red Poor Wine (N=741)** (4) \n",
    "* r2 on the test = 0.083575\n",
    "* Features: Volatile Acidity, Total Sulfur Dioxide, pH, Citric Acid, Alcohol, Sulphates, Density, Residual Sugar.\n",
    "\n",
    "**Red Good Wine (N=851)** (6)\n",
    "* r2 on the test = 0.147937\n",
    "* Features: Alcohol, volatile acidity, sulphates, chlorides, total sulfur dioxide, residual sugar, pH\n",
    "\n",
    "**Comment**: Overall, we can say that the models don't predict quality very much, r2 suggest that they describe between 13% to 32% of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When it comes to Intercept and coeficients scores of the model there are presented below: \n",
    "\n",
    "e.g. Pipeline_White Wine Poor (Without Outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>const</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>Intercept</td>\n",
       "      <td>4.882263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>free sulfur dioxide</td>\n",
       "      <td>0.037150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>volatile acidity</td>\n",
       "      <td>-0.054437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>residual sugar</td>\n",
       "      <td>0.022813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>alcohol</td>\n",
       "      <td>-0.017102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>fixed acidity</td>\n",
       "      <td>-0.024791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>citric acid</td>\n",
       "      <td>0.016143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>chlorides</td>\n",
       "      <td>-0.007053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>Pipeline_White Wine Poor (Without Outliers)</td>\n",
       "      <td>sulphates</td>\n",
       "      <td>0.003767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            df  \\\n",
       "0  Pipeline_White Wine Poor (Without Outliers)   \n",
       "1  Pipeline_White Wine Poor (Without Outliers)   \n",
       "2  Pipeline_White Wine Poor (Without Outliers)   \n",
       "3  Pipeline_White Wine Poor (Without Outliers)   \n",
       "4  Pipeline_White Wine Poor (Without Outliers)   \n",
       "5  Pipeline_White Wine Poor (Without Outliers)   \n",
       "6  Pipeline_White Wine Poor (Without Outliers)   \n",
       "7  Pipeline_White Wine Poor (Without Outliers)   \n",
       "8  Pipeline_White Wine Poor (Without Outliers)   \n",
       "\n",
       "                                         const           Parameters  \\\n",
       "0  Pipeline_White Wine Poor (Without Outliers)            Intercept   \n",
       "1  Pipeline_White Wine Poor (Without Outliers)  free sulfur dioxide   \n",
       "2  Pipeline_White Wine Poor (Without Outliers)     volatile acidity   \n",
       "3  Pipeline_White Wine Poor (Without Outliers)       residual sugar   \n",
       "4  Pipeline_White Wine Poor (Without Outliers)              alcohol   \n",
       "5  Pipeline_White Wine Poor (Without Outliers)        fixed acidity   \n",
       "6  Pipeline_White Wine Poor (Without Outliers)          citric acid   \n",
       "7  Pipeline_White Wine Poor (Without Outliers)            chlorides   \n",
       "8  Pipeline_White Wine Poor (Without Outliers)            sulphates   \n",
       "\n",
       "   Coefficients  \n",
       "0      4.882263  \n",
       "1      0.037150  \n",
       "2     -0.054437  \n",
       "3      0.022813  \n",
       "4     -0.017102  \n",
       "5     -0.024791  \n",
       "6      0.016143  \n",
       "7     -0.007053  \n",
       "8      0.003767  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_matrix_list[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** based on the coeddicient we can say that the most importnat feature for White poor wine quality prediction is volatile acidity (|score| = 0.05). However, because the model doesn't provide statistical calcualtion od t value and p-value, we don't know if the score is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Importance \n",
    "\n",
    "Besides using Linear Regression Model, we realised later on that it might be a better idea to use Random Forest to get important features in our receipes. \n",
    "\n",
    "The following code is a rudimentary attempt to train a Random Forest Classifier on different subsets of wine data and visualizing the importance of various features in predicting wine quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def get_feature_importances(dataframe):\n",
    "    numeric_columns = dataframe.select_dtypes(include=np.number).columns.tolist()\n",
    "    if 'quality' in numeric_columns:\n",
    "        numeric_columns.remove('quality')\n",
    "    X = dataframe[numeric_columns]\n",
    "    y = dataframe['quality']\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    return rf.feature_importances_, numeric_columns\n",
    "\n",
    "\n",
    "good_red_importances, good_red_columns = get_feature_importances(df_red_good_without_outliers)\n",
    "poor_red_importances, poor_red_columns = get_feature_importances(df_red_poor_without_outliers)\n",
    "good_white_importances, good_white_columns = get_feature_importances(df_white_good_without_outliers)\n",
    "poor_white_importances, poor_white_columns = get_feature_importances(df_white_poor_without_outliers)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "axes[0].bar(good_red_columns, good_red_importances)\n",
    "axes[1].bar(poor_red_columns, poor_red_importances)\n",
    "axes[2].bar(good_white_columns, good_white_importances)\n",
    "axes[3].bar(poor_white_columns, poor_white_importances)\n",
    "\n",
    "for i, df, columns in zip(range(4), \n",
    "                          [df_red_good_without_outliers, df_red_poor_without_outliers, df_white_good_without_outliers, df_white_poor_without_outliers], \n",
    "                          [good_red_columns, poor_red_columns, good_white_columns, poor_white_columns]):\n",
    "    axes[i].set_title(f\"{'Good' if i%2 == 0 else 'Poor'} {'Red' if i < 2 else 'White'} Wine\")\n",
    "    axes[i].set_xticks(range(len(columns)))\n",
    "    axes[i].set_xticklabels(columns, rotation=45, ha=\"right\")\n",
    "    axes[i].annotate(f\"n = {len(df)}\", xy=(0.5, 1), xycoords='axes fraction', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
